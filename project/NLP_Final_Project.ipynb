{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Corpus / Packages"
      ],
      "metadata": {
        "id": "WbVF0NggGoHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO9HWbvErHiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3fdd82-931d-4a2b-c18a-612c1b4d872a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-04 18:47:54--  https://archive.org/download/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia903107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip [following]\n",
            "--2024-06-04 18:47:54--  https://ia903107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving ia903107.us.archive.org (ia903107.us.archive.org)... 207.241.232.147\n",
            "Connecting to ia903107.us.archive.org (ia903107.us.archive.org)|207.241.232.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 364888893 (348M) [application/zip]\n",
            "Saving to: ‘wiki_cbow_300.zip’\n",
            "\n",
            "wiki_cbow_300.zip   100%[===================>] 347.98M   678KB/s    in 5m 0s   \n",
            "\n",
            "2024-06-04 18:52:54 (1.16 MB/s) - ‘wiki_cbow_300.zip’ saved [364888893/364888893]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://archive.org/download/aravec2.0/wiki_cbow_300.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_sg_300_wiki.zip'"
      ],
      "metadata": {
        "id": "xpCZyQ3qa1h_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29071961-79e9-44dc-bd39-fba976446f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-04 18:53:01--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_sg_300_wiki.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1488871452 (1.4G) [application/zip]\n",
            "Saving to: ‘full_grams_sg_300_wiki.zip’\n",
            "\n",
            "full_grams_sg_300_w 100%[===================>]   1.39G  95.0MB/s    in 17s     \n",
            "\n",
            "2024-06-04 18:53:19 (82.0 MB/s) - ‘full_grams_sg_300_wiki.zip’ saved [1488871452/1488871452]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/full_grams_sg_300_wiki.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQdnbY3LT7pE",
        "outputId": "d1d48b1b-2a7d-4ff2-e02f-39aac0d76ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/full_grams_sg_300_wiki.zip\n",
            "  inflating: full_grams_sg_300_wiki.mdl  \n",
            "  inflating: full_grams_sg_300_wiki.mdl.trainables.syn1neg.npy   bad CRC 025479a0  (should be 55675a21)\n",
            "  inflating: full_grams_sg_300_wiki.mdl.wv.vectors.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMB3HX_ArHl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9dfbcc-5073-429c-ad70-cacc349c9b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wiki_cbow_300.zip\n",
            "  inflating: wikipedia_cbow_300      \n",
            "  inflating: wikipedia_cbow_300.trainables.syn1neg.npy  \n",
            "  inflating: wikipedia_cbow_300.wv.vectors.npy  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"wiki_cbow_300.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhtoVHDmRexS",
        "outputId": "6fa58eaf-cea7-490f-c39b-4d038e709e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import nltk.data\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import ISRIStemmer\n",
        "stemmer = ISRIStemmer()\n",
        "stemmer2 = SnowballStemmer('arabic')\n",
        "stemmer3 = PorterStemmer()\n",
        "stopwords = stopwords.words('arabic')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Techniques"
      ],
      "metadata": {
        "id": "iPHBfSh4Gwiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ev_kFLzql3mw",
        "outputId": "94befbde-e7f2-4a23-a1e8-6459d3046b87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Type                                               News\n",
              "0  politics  اشتباك الحريري-عون.. اتهامات لباسيل بالتمسك با...\n",
              "1  politics                     عون: الحريري أصبح غريب الأطوار\n",
              "2  politics  وزير الخارجية الأمريكي: ندرس سحب كامل قواتنا م...\n",
              "3  politics  أفغانستان.. استعدادات حثيثة لاجتماع تركيا وكاب...\n",
              "4  politics  أندبندنت‮:‬ ‬مفاوضات ‬سرّية ‬‬كادت ‬تنقذ ‬القذافي"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a83c9dbd-5e55-455e-aec1-cbcb8f622e4a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politics</td>\n",
              "      <td>اشتباك الحريري-عون.. اتهامات لباسيل بالتمسك با...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>politics</td>\n",
              "      <td>عون: الحريري أصبح غريب الأطوار</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>politics</td>\n",
              "      <td>وزير الخارجية الأمريكي: ندرس سحب كامل قواتنا م...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>politics</td>\n",
              "      <td>أفغانستان.. استعدادات حثيثة لاجتماع تركيا وكاب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>politics</td>\n",
              "      <td>أندبندنت‮:‬ ‬مفاوضات ‬سرّية ‬‬كادت ‬تنقذ ‬القذافي</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a83c9dbd-5e55-455e-aec1-cbcb8f622e4a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a83c9dbd-5e55-455e-aec1-cbcb8f622e4a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a83c9dbd-5e55-455e-aec1-cbcb8f622e4a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e6bb6663-bb12-4110-8d2e-4d33db10828a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6bb6663-bb12-4110-8d2e-4d33db10828a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e6bb6663-bb12-4110-8d2e-4d33db10828a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"economic\",\n          \"tech\",\n          \"politics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"News\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4951,\n        \"samples\": [\n          \"\\u0648\\u0641\\u0627\\u0629 \\u0631\\u0626\\u064a\\u0633 \\u062a\\u0646\\u0632\\u0627\\u0646\\u064a\\u0627 \\u0628\\u0639\\u062f \\u0641\\u062a\\u0631\\u0629 \\u063a\\u064a\\u0627\\u0628 \\u063a\\u0627\\u0645\\u0636\\u0629 \\u0639\\u0646 \\u0639\\u0645\\u0631 \\u0646\\u0627\\u0647\\u0632 61 \\u0639\\u0627\\u0645\\u0627\",\n          \"\\u0627\\u0644\\u0648\\u062f\\u0627\\u062f \\u064a\\u062a\\u0623\\u0647\\u0644 \\u0631\\u0633\\u0645\\u064a\\u064b\\u0627 \\u0625\\u0644\\u0649 \\u0631\\u0628\\u0639 \\u0646\\u0647\\u0627\\u0626\\u0649 \\u062f\\u0648\\u0631\\u0649 \\u0623\\u0628\\u0637\\u0627\\u0644 \\u0625\\u0641\\u0631\\u064a\\u0642\\u064a\\u0627 \\u0628\\u0639\\u062f \\u0627\\u0644\\u062a\\u0639\\u0627\\u062f\\u0644 \\u0645\\u0639 \\u062d\\u0648\\u0631\\u064a\\u0627\",\n          \"\\u062a\\u0634\\u064a\\u0644\\u0633\\u064a \\u064a\\u0647\\u0632\\u0645 \\u0648\\u0633\\u062a \\u0647\\u0627\\u0645 \\u0648\\u064a\\u0639\\u0632\\u0632 \\u0645\\u0648\\u0642\\u0639\\u0647 \\u062f\\u0627\\u062e\\u0644 \\u0627\\u0644\\u0645\\u0631\\u0628\\u0639 \\u0627\\u0644\\u0630\\u0647\\u0628\\u064a \\u0644\\u0640\\u00ab\\u0628\\u0631\\u064a\\u0645\\u064a\\u0631\\u0644\\u064a\\u062c\\u00bb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "excel = pd.read_excel('/content/drive/MyDrive/News_train.xlsx')\n",
        "\n",
        "df = pd.DataFrame(excel)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "NKw44Pm6EKdk",
        "outputId": "6441afb8-ee4c-4d13-8cec-db1e37dc7a5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Type', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(x='Type', data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m993MuQFjxX"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = text.strip()\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  text = re.sub('أ|إ|آ|ا', 'ا', text)\n",
        "  text = re.sub('ال', '', text)\n",
        "  text = re.sub('ة', 'ه', text)\n",
        "  text = re.sub(r'[a-zA-Z]' , '', text)\n",
        "  return text\n",
        "\n",
        "df['News_processed'] = df['News'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tYV8yJGmFjwm",
        "outputId": "c5f0b11c-6ea7-4348-ea3a-19588f1cb18d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'سيف قذافي اتفاق قريب جدا على ترحيل ممرضات وطبيب بلغاريين'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "(df['News_processed'][5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Np-u8tKvV9g"
      },
      "outputs": [],
      "source": [
        "def stem_and_tokenize_arabic(text):\n",
        "  stemmed_text = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "  processed_text = ' '.join(stemmed_text)\n",
        "  return processed_text\n",
        "\n",
        "df['News_processed'] = df['News_processed'].apply(stem_and_tokenize_arabic)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_and_tokenize_arabic(text):\n",
        "  stemmed_text = [stemmer2.stem(word) for word in word_tokenize(text)]\n",
        "  processed_text = ' '.join(stemmed_text)\n",
        "  return processed_text\n",
        "\n",
        "df['News_processed'] = df['News_processed'].apply(stem_and_tokenize_arabic)\n"
      ],
      "metadata": {
        "id": "GhsaLnCs37Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_and_tokenize_arabic(text):\n",
        "  stemmed_text = [stemmer3.stem(word) for word in word_tokenize(text)]\n",
        "  processed_text = ' '.join(stemmed_text)\n",
        "  return processed_text\n",
        "\n",
        "df['News_processed'] = df['News_processed'].apply(stem_and_tokenize_arabic)\n"
      ],
      "metadata": {
        "id": "-_B4mZLK37M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF and BOW implementation"
      ],
      "metadata": {
        "id": "GKgNuNejG1FC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O881AcOqfMhe"
      },
      "outputs": [],
      "source": [
        "x = df.drop(['Type', 'News'], axis = 1)\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "from keras.utils import to_categorical\n",
        "y = to_categorical(labels)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0catqBySfJL",
        "outputId": "1a4e2f48-dc72-4bef-fccd-402bc1bba1e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4000x500 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 27050 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 500)\n",
        "\n",
        "x_train_tfidf = vectorizer.fit_transform(x_train['News_processed'])\n",
        "x_val_tfidf = vectorizer.fit_transform(x_test['News_processed'])\n",
        "\n",
        "train_tfidf = pd.DataFrame(x_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "val_tfidf = pd.DataFrame(x_val_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "x_train_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = cv_uni_vec = CountVectorizer(max_features=500,\n",
        "                                 stop_words=None,\n",
        "                                 ngram_range=(2,2)\n",
        "                                 )\n",
        "X_train_uni = cv_uni_vec.fit_transform(x_train['News_processed'])\n",
        "X_test_uni = cv_uni_vec.fit_transform(x_test['News_processed'])\n",
        "\n",
        "train_bow = pd.DataFrame(X_train_uni.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "val_bow = pd.DataFrame(X_test_uni.toarray(), columns=vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "XfjL7A7e8vu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_uni.toarray()\n",
        "\n",
        "X_train_uni.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6czc_769g4-",
        "outputId": "3a217d78-fda8-4355-deb8-89c104a6334a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twk3IW5Z9g7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymN5Cy6deiyJ",
        "outputId": "de73e8cd-4650-4fb9-c370-503acb38753c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.35581571],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "x_train_tfidf.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg2zNrl_tx0j",
        "outputId": "9450bf62-a3d8-4b46-a35e-9c023402c1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF & BOW with LR and NB"
      ],
      "metadata": {
        "id": "tDGnWsM7HCWr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcgeTljseMRK",
        "outputId": "803b9c3f-d6be-443b-dab6-cb914122c08e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.50\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.69      0.69       647\n",
            "           1       0.16      0.16      0.16       163\n",
            "           2       0.15      0.14      0.15       176\n",
            "           3       0.12      0.07      0.09        14\n",
            "\n",
            "    accuracy                           0.50      1000\n",
            "   macro avg       0.28      0.27      0.27      1000\n",
            "weighted avg       0.49      0.50      0.50      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "naive_bayes_classifier.fit(train_tfidf.values, y2_train)\n",
        "\n",
        "y_pred = naive_bayes_classifier.predict(val_tfidf.values)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(train_tfidf.values, y2_train)\n",
        "\n",
        "y_pred = model.predict(val_tfidf.values)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-f8wHl92gQL",
        "outputId": "57c617c2-a794-46c0-be8f-7bb0e3b6de92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.59\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.83      0.75       647\n",
            "           1       0.16      0.10      0.13       163\n",
            "           2       0.35      0.20      0.26       176\n",
            "           3       0.00      0.00      0.00        14\n",
            "\n",
            "    accuracy                           0.59      1000\n",
            "   macro avg       0.30      0.29      0.28      1000\n",
            "weighted avg       0.53      0.59      0.55      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_03JyZtvLiZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d0692e-1216-4f70-ea5d-398e10c07a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.34\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.42      0.52       647\n",
            "           1       0.14      0.18      0.16       163\n",
            "           2       0.20      0.16      0.18       176\n",
            "           3       0.05      0.86      0.09        14\n",
            "\n",
            "    accuracy                           0.34      1000\n",
            "   macro avg       0.27      0.40      0.24      1000\n",
            "weighted avg       0.51      0.34      0.40      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "naive_bayes_classifier.fit(train_bow.values, y2_train)\n",
        "\n",
        "y_pred = naive_bayes_classifier.predict(val_bow.values)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(train_bow.values, y2_train)\n",
        "\n",
        "y_pred = model.predict(val_bow.values)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E1XBcBP-LxB",
        "outputId": "3e0ef71d-64c8-4406-a188-7b3e94a2f0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.55\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.80      0.71       647\n",
            "           1       0.19      0.12      0.15       163\n",
            "           2       0.12      0.05      0.07       176\n",
            "           3       0.00      0.00      0.00        14\n",
            "\n",
            "    accuracy                           0.55      1000\n",
            "   macro avg       0.24      0.24      0.23      1000\n",
            "weighted avg       0.46      0.55      0.50      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W2V CBOW with NB and LR"
      ],
      "metadata": {
        "id": "cZOLvP5kHLi3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRbkZHGDSfKs",
        "outputId": "128df1de-d9ca-4cb2-c508-ae726e223dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load('/content/wikipedia_cbow_300')\n",
        "embedding_dim = model.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNAP3eg3ZhYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "1e0d539c-2892-4d71-b706-5dc784ccca97"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'le' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ffaa039f6358>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'le' is not defined"
          ]
        }
      ],
      "source": [
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYZD-tIULSS9"
      },
      "outputs": [],
      "source": [
        "OOV_tokens = []\n",
        "train_tokens = []\n",
        "test_tokens = []\n",
        "train_w2v_embeddings = []\n",
        "test_w2v_embeddings = []\n",
        "\n",
        "def get_doc_vec(sent, model, data):\n",
        "  w2v_embeddings = []\n",
        "  tokens = sent.split()\n",
        "  for word in tokens:\n",
        "    try:\n",
        "      if data == 'train':\n",
        "        w2v_embeddings.append(model.wv[word])\n",
        "        train_tokens.append(word)\n",
        "      else:\n",
        "        w2v_embeddings.append(model.wv[word])\n",
        "        test_tokens.append(word)\n",
        "    except:\n",
        "      OOV_tokens.append(word)\n",
        "      continue\n",
        "  if len(w2v_embeddings) == 0:\n",
        "    return None\n",
        "  return sum(w2v_embeddings)/len(w2v_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wam7TsbROZ4S"
      },
      "outputs": [],
      "source": [
        "X_train_w2v_embeddings = x_train['News_processed'].apply(lambda sent: get_doc_vec(sent, model,'train'))\n",
        "X_test_w2v_embeddings = x_test['News_processed'].apply(lambda sent: get_doc_vec(sent, model,'test'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXshVqi_OZ6e"
      },
      "outputs": [],
      "source": [
        "X_train_w2v_c_embeddings_list = []\n",
        "X_test_w2v_c_embeddings_list = []\n",
        "\n",
        "for embedding in X_train_w2v_embeddings:\n",
        "    if embedding is not None:\n",
        "        X_train_w2v_c_embeddings_list.append(embedding)\n",
        "    else:\n",
        "        X_train_w2v_c_embeddings_list.append(np.zeros(embedding_dim))\n",
        "\n",
        "for embedding in X_test_w2v_embeddings:\n",
        "    if embedding is not None:\n",
        "        X_test_w2v_c_embeddings_list.append(embedding)\n",
        "    else:\n",
        "        X_test_w2v_c_embeddings_list.append(np.zeros(embedding_dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "naive_bayes_classifier.fit(X_train_w2v_c_embeddings_list, y2_train)\n",
        "\n",
        "y_pred = naive_bayes_classifier.predict(X_test_w2v_c_embeddings_list)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhVqC3eY6eVF",
        "outputId": "5ac8f2ff-14ee-41f5-d002-57fdcf192b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.76\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       647\n",
            "           1       0.69      0.72      0.71       163\n",
            "           2       0.67      0.57      0.62       176\n",
            "           3       0.14      0.57      0.22        14\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.59      0.67      0.60      1000\n",
            "weighted avg       0.79      0.76      0.77      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_w2v_c_embeddings_list, y2_train)\n",
        "\n",
        "y_pred = model.predict(X_test_w2v_c_embeddings_list)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmIagVx_3QM5",
        "outputId": "4ae8661a-3841-4a81-bd3e-321d918e289a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       647\n",
            "           1       0.82      0.82      0.82       163\n",
            "           2       0.82      0.69      0.75       176\n",
            "           3       0.38      0.36      0.37        14\n",
            "\n",
            "    accuracy                           0.86      1000\n",
            "   macro avg       0.73      0.70      0.71      1000\n",
            "weighted avg       0.85      0.86      0.85      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W2V SG with NB and LR"
      ],
      "metadata": {
        "id": "w8qRMZtlHUtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load('/content/full_grams_sg_300_wiki.mdl')\n",
        "embedding_dim = model.vector_size"
      ],
      "metadata": {
        "id": "zR_-erJYUFu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOV_tokens = []\n",
        "train_tokens = []\n",
        "test_tokens = []\n",
        "train_w2v_embeddings = []\n",
        "test_w2v_embeddings = []\n",
        "\n",
        "def get_doc_vec(sent, model, data):\n",
        "  w2v_embeddings = []\n",
        "  tokens = sent.split()\n",
        "  for word in tokens:\n",
        "    try:\n",
        "      if data == 'train':\n",
        "        w2v_embeddings.append(model.wv[word])\n",
        "        train_tokens.append(word)\n",
        "      else:\n",
        "        w2v_embeddings.append(model.wv[word])\n",
        "        test_tokens.append(word)\n",
        "    except:\n",
        "      OOV_tokens.append(word)\n",
        "      continue\n",
        "  if len(w2v_embeddings) == 0:\n",
        "    return None\n",
        "  return sum(w2v_embeddings)/len(w2v_embeddings)"
      ],
      "metadata": {
        "id": "_GaUrsDZUWoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v_embeddings = x_train['News_processed'].apply(lambda sent: get_doc_vec(sent, model,'train'))\n",
        "X_test_w2v_embeddings = x_test['News_processed'].apply(lambda sent: get_doc_vec(sent, model,'test'))"
      ],
      "metadata": {
        "id": "MTAZWnBnUWr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v_sg_embeddings_list = []\n",
        "X_test_w2v_sg_embeddings_list = []\n",
        "\n",
        "for embedding in X_train_w2v_embeddings:\n",
        "    if embedding is not None:\n",
        "        X_train_w2v_sg_embeddings_list.append(embedding)\n",
        "    else:\n",
        "        X_train_w2v_sg_embeddings_list.append(np.zeros(embedding_dim))\n",
        "\n",
        "for embedding in X_test_w2v_embeddings:\n",
        "    if embedding is not None:\n",
        "        X_test_w2v_sg_embeddings_list.append(embedding)\n",
        "    else:\n",
        "        X_test_w2v_sg_embeddings_list.append(np.zeros(embedding_dim))"
      ],
      "metadata": {
        "id": "hUd-7WPMUF0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v_embeddings_array = np.array(X_train_w2v_sg_embeddings_list)\n",
        "X_test_w2v_embeddings_array = np.array(X_test_w2v_sg_embeddings_list)"
      ],
      "metadata": {
        "id": "ZOMV6Q2Rkdl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XgngAW4qov",
        "outputId": "d004f2f1-60e3-48e8-f150-0bf932e62412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.87       647\n",
            "           1       0.74      0.82      0.78       163\n",
            "           2       0.75      0.66      0.70       176\n",
            "           3       0.14      0.36      0.20        14\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.63      0.67      0.64      1000\n",
            "weighted avg       0.82      0.81      0.81      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "naive_bayes_classifier.fit(X_train_w2v_sg_embeddings_list, y2_train)\n",
        "\n",
        "y_pred = naive_bayes_classifier.predict(X_test_w2v_sg_embeddings_list)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_w2v_sg_embeddings_list, y2_train)\n",
        "\n",
        "y_pred = model.predict(X_test_w2v_sg_embeddings_list)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "id": "fzgwcWsA3bBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919ddfdb-5a23-4c53-ad5f-04ecca07e5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92       647\n",
            "           1       0.89      0.87      0.88       163\n",
            "           2       0.87      0.70      0.77       176\n",
            "           3       1.00      0.07      0.13        14\n",
            "\n",
            "    accuracy                           0.88      1000\n",
            "   macro avg       0.91      0.65      0.68      1000\n",
            "weighted avg       0.88      0.88      0.87      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Embedding with LR and NB & Model"
      ],
      "metadata": {
        "id": "7stRpjjxHZdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeyAQz9MOa-8",
        "outputId": "07c5454b-2688-4b39-af4a-4ec96d02a32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMDhK2pLSfUq",
        "outputId": "42ce910e-cca5-4f0b-ee35-314100bfc482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 768)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model_name = 'aubmindlab/bert-base-arabert'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "embeddings_list = []\n",
        "max_length = 0\n",
        "for word in df['News_processed']:\n",
        "  tokens = tokenizer.tokenize(word)\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  max_length = max(max_length, len(token_ids))\n",
        "  embeddings_list.append(token_ids)\n",
        "\n",
        "padded_embeddings_list = []\n",
        "for tokens in embeddings_list:\n",
        "    padded_tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n",
        "    padded_embeddings_list.append(padded_tokens)\n",
        "\n",
        "tensor_token_ids = torch.tensor(padded_embeddings_list)\n",
        "\n",
        "with torch.no_grad():\n",
        "    results = model(tensor_token_ids)\n",
        "\n",
        "embeddings = results.last_hidden_state\n",
        "\n",
        "mean_embeddings = torch.mean(embeddings, dim=1)\n",
        "embeddings_list = mean_embeddings.numpy()\n",
        "\n",
        "print(embeddings_list.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOR-bSgoV8Gv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "from keras.utils import to_categorical\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings_list, y, test_size=0.2, random_state=42)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(embeddings_list, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X2_train = np.array(X2_train)\n",
        "X2_test = np.array(X2_test)\n",
        "y2_train = np.array(y2_train)\n",
        "y2_test = np.array(y2_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_name = 'aubmindlab/bert-base-arabert'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_len):\n",
        "    tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "train_sentences = x_train['News_processed'].tolist()\n",
        "test_sentences = x_test['News_processed'].tolist()\n",
        "\n",
        "max_sequence_len = max(len(x.split()) for x in train_sentences + test_sentences)  # maximum length of sequences\n",
        "train_input_ids, train_attention_mask = tokenize(train_sentences, tokenizer, max_sequence_len)\n",
        "test_input_ids, test_attention_mask = tokenize(test_sentences, tokenizer, max_sequence_len)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y2_train)\n",
        "y_test_encoded = label_encoder.transform(y2_test)\n",
        "\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, torch.tensor(y_train_encoded))\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_mask, torch.tensor(y_test_encoded))\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Generate classification report\n",
        "target_names = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
        "target_names = [str(name) for name in target_names]  # Convert target names to strings\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "PyG-UXli65nV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a9af29-a763-4e71-bf71-16584823e5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Accuracy: 0.84\n",
            "Epoch 2/5, Accuracy: 0.89\n",
            "Epoch 3/5, Accuracy: 0.88\n",
            "Epoch 4/5, Accuracy: 0.90\n",
            "Epoch 5/5, Accuracy: 0.90\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.93       647\n",
            "           1       0.91      0.92      0.91       163\n",
            "           2       0.86      0.72      0.78       176\n",
            "           3       0.56      0.36      0.43        14\n",
            "\n",
            "    accuracy                           0.90      1000\n",
            "   macro avg       0.81      0.74      0.76      1000\n",
            "weighted avg       0.89      0.90      0.89      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y2_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8-ZKHm9FyS",
        "outputId": "3420b140-fb95-45e2-8676-a7264a3969ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.51\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.46      0.59       647\n",
            "           1       0.52      0.61      0.56       163\n",
            "           2       0.25      0.62      0.36       176\n",
            "           3       0.10      0.14      0.11        14\n",
            "\n",
            "    accuracy                           0.51      1000\n",
            "   macro avg       0.43      0.46      0.41      1000\n",
            "weighted avg       0.67      0.51      0.54      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X2_train, y2_train)\n",
        "\n",
        "y_pred = model.predict(X2_test)\n",
        "\n",
        "accuracy = accuracy_score(y2_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y2_test, y_pred))"
      ],
      "metadata": {
        "id": "sTL_CYIT35xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eeaaa88-7dac-4205-ecd7-5e6ef8d9841d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.82\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       647\n",
            "           1       0.80      0.77      0.78       163\n",
            "           2       0.73      0.65      0.69       176\n",
            "           3       0.11      0.07      0.09        14\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.62      0.60      0.61      1000\n",
            "weighted avg       0.81      0.82      0.81      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert embedding with GRU & LSTM"
      ],
      "metadata": {
        "id": "rSoj0FHTHs7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S__Pl429V8J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f29b03a-5126-4fed-ca48-4f27dd3f3e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 1.0387 - accuracy: 0.6047 - val_loss: 0.8923 - val_accuracy: 0.6640\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.8998 - accuracy: 0.6543 - val_loss: 0.8253 - val_accuracy: 0.7030\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.7740 - accuracy: 0.7090 - val_loss: 0.7445 - val_accuracy: 0.7260\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.7395 - val_loss: 0.5984 - val_accuracy: 0.7870\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6141 - accuracy: 0.7655 - val_loss: 0.6074 - val_accuracy: 0.8010\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5162 - accuracy: 0.8152 - val_loss: 0.5256 - val_accuracy: 0.8150\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4798 - accuracy: 0.8292 - val_loss: 0.5354 - val_accuracy: 0.8100\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.4238 - accuracy: 0.8465 - val_loss: 0.4945 - val_accuracy: 0.8290\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.3865 - accuracy: 0.8600 - val_loss: 0.4942 - val_accuracy: 0.8250\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.3641 - accuracy: 0.8677 - val_loss: 0.5865 - val_accuracy: 0.7920\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87       647\n",
            "           1       0.59      0.93      0.72       163\n",
            "           2       0.83      0.47      0.60       176\n",
            "           3       0.00      0.00      0.00        14\n",
            "\n",
            "    accuracy                           0.79      1000\n",
            "   macro avg       0.57      0.57      0.55      1000\n",
            "weighted avg       0.81      0.79      0.79      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)  # maximum length of sequences\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(5000, 768, weights = [embeddings_list], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(GRU(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(max_sequence_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkmttZdmsFJf",
        "outputId": "b6178ccb-b9c4-4f55-86c4-129a4e95ce38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_padded[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0vXd_ZUr0Kx",
        "outputId": "d31af7b5-bfb5-45af-cc17-4815d04366b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,   23,   31,  660,    2,\n",
              "         65,  336, 1093,    1,  167], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "embeddings_array = np.array(embeddings_list)\n",
        "bert_embeddings = embeddings_array.reshape((5000,768))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)  # maximum length of sequences\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(5000, 768, weights = [bert_embeddings], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(LSTM(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG8KWDM_dOZU",
        "outputId": "9beb8f43-6fa9-4bb9-badf-5bceafd1072e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 1.0151 - accuracy: 0.6140 - val_loss: 0.9069 - val_accuracy: 0.6630\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9562 - accuracy: 0.6323 - val_loss: 0.8859 - val_accuracy: 0.6630\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.9357 - accuracy: 0.6390 - val_loss: 0.8778 - val_accuracy: 0.6640\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8961 - accuracy: 0.6530 - val_loss: 0.8453 - val_accuracy: 0.6730\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8545 - accuracy: 0.6700 - val_loss: 0.7955 - val_accuracy: 0.7030\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.7768 - accuracy: 0.6975 - val_loss: 0.7745 - val_accuracy: 0.6970\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.7139 - accuracy: 0.7243 - val_loss: 0.6770 - val_accuracy: 0.7250\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6611 - accuracy: 0.7455 - val_loss: 0.7494 - val_accuracy: 0.7060\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.7705 - val_loss: 0.5886 - val_accuracy: 0.7820\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.5967 - accuracy: 0.7760 - val_loss: 0.5582 - val_accuracy: 0.8010\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       647\n",
            "           1       0.80      0.77      0.78       163\n",
            "           2       0.67      0.51      0.58       176\n",
            "           3       0.00      0.00      0.00        14\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.57      0.54      0.56      1000\n",
            "weighted avg       0.78      0.80      0.79      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSg2A9JJRZWN",
        "outputId": "7e072537-97ee-40ec-ce1e-d33d186be913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.3014416 ,  1.0202115 ,  0.49604067, ..., -0.0678372 ,\n",
              "         0.29666933,  0.11980575],\n",
              "       [-0.3497997 ,  1.1660739 ,  0.59739494, ...,  0.7400403 ,\n",
              "        -0.11763303,  0.24691655],\n",
              "       [-0.08963694,  0.75863695,  0.3407913 , ...,  0.26959124,\n",
              "         0.15297945,  0.433246  ],\n",
              "       ...,\n",
              "       [-0.27660304,  1.0125841 ,  0.35874498, ...,  0.34917942,\n",
              "         0.24367024,  0.02161656],\n",
              "       [-0.27867278,  1.1481411 ,  0.55462   , ...,  0.2817615 ,\n",
              "         0.1840114 ,  0.644825  ],\n",
              "       [-0.16831407,  1.0457609 ,  0.43612203, ...,  0.10131398,\n",
              "         0.12891287,  0.3510527 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "embeddings_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF & BOW with GRU & LSTM"
      ],
      "metadata": {
        "id": "hxVlez7eH1p1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8z2RwPQAII7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501c0f2d-dcc0-483c-84a7-2e1fa990e4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 4s 15ms/step - loss: 0.9383 - accuracy: 0.6290 - val_loss: 0.8363 - val_accuracy: 0.7310\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5707 - accuracy: 0.7925 - val_loss: 0.5020 - val_accuracy: 0.8260\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4117 - accuracy: 0.8535 - val_loss: 0.4213 - val_accuracy: 0.8500\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3510 - accuracy: 0.8708 - val_loss: 0.4329 - val_accuracy: 0.8550\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2967 - accuracy: 0.8942 - val_loss: 0.4652 - val_accuracy: 0.8330\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2438 - accuracy: 0.9158 - val_loss: 0.4497 - val_accuracy: 0.8480\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2140 - accuracy: 0.9337 - val_loss: 0.4694 - val_accuracy: 0.8570\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1680 - accuracy: 0.9482 - val_loss: 0.5297 - val_accuracy: 0.8250\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1329 - accuracy: 0.9578 - val_loss: 0.5325 - val_accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1104 - accuracy: 0.9630 - val_loss: 0.5423 - val_accuracy: 0.8420\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.89       647\n",
            "           1       0.84      0.89      0.86       163\n",
            "           2       0.71      0.67      0.69       176\n",
            "           3       0.25      0.14      0.18        14\n",
            "\n",
            "    accuracy                           0.84      1000\n",
            "   macro avg       0.67      0.65      0.66      1000\n",
            "weighted avg       0.84      0.84      0.84      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)  # maximum length of sequences\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 500, weights = [x_train_tfidf.toarray()], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(GRU(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "trEgMN_yFhx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)  # maximum length of sequences\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 500, weights = [X_train_uni.toarray()], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(GRU(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFZ16XCW_M1X",
        "outputId": "f000f15e-e46b-40af-e07a-ed7f8439f152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 0.9146 - accuracy: 0.6447 - val_loss: 0.6618 - val_accuracy: 0.7470\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7880 - val_loss: 0.4971 - val_accuracy: 0.8150\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4566 - accuracy: 0.8332 - val_loss: 0.4750 - val_accuracy: 0.8430\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.3977 - accuracy: 0.8575 - val_loss: 0.4982 - val_accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3570 - accuracy: 0.8700 - val_loss: 0.4937 - val_accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.3047 - accuracy: 0.8917 - val_loss: 0.5315 - val_accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2649 - accuracy: 0.9068 - val_loss: 0.5214 - val_accuracy: 0.8330\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2289 - accuracy: 0.9190 - val_loss: 0.5259 - val_accuracy: 0.8220\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2035 - accuracy: 0.9280 - val_loss: 0.5565 - val_accuracy: 0.8310\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1769 - accuracy: 0.9392 - val_loss: 0.5706 - val_accuracy: 0.8290\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88       647\n",
            "           1       0.75      0.83      0.79       163\n",
            "           2       0.72      0.67      0.70       176\n",
            "           3       0.75      0.21      0.33        14\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.78      0.65      0.68      1000\n",
            "weighted avg       0.83      0.83      0.83      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)  # maximum length of sequences\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 500, weights = [X_train_uni.toarray()], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(LSTM(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq8UvU3I_M3l",
        "outputId": "2bbff0de-96b1-427d-dbc7-4e3ccdc25801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 0.9664 - accuracy: 0.6225 - val_loss: 0.8041 - val_accuracy: 0.7210\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6343 - accuracy: 0.7665 - val_loss: 0.5422 - val_accuracy: 0.8100\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4928 - accuracy: 0.8220 - val_loss: 0.5254 - val_accuracy: 0.8170\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4350 - accuracy: 0.8410 - val_loss: 0.4940 - val_accuracy: 0.8300\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3764 - accuracy: 0.8652 - val_loss: 0.4753 - val_accuracy: 0.8420\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3413 - accuracy: 0.8750 - val_loss: 0.4924 - val_accuracy: 0.8240\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3063 - accuracy: 0.8857 - val_loss: 0.5014 - val_accuracy: 0.8340\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.2630 - accuracy: 0.9032 - val_loss: 0.5226 - val_accuracy: 0.8250\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.2342 - accuracy: 0.9137 - val_loss: 0.5881 - val_accuracy: 0.8290\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2149 - accuracy: 0.9225 - val_loss: 0.5570 - val_accuracy: 0.8290\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89       647\n",
            "           1       0.80      0.79      0.79       163\n",
            "           2       0.72      0.63      0.67       176\n",
            "           3       0.33      0.29      0.31        14\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.68      0.65      0.67      1000\n",
            "weighted avg       0.82      0.83      0.83      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)  # maximum length of sequences\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 500, weights = [x_train_tfidf.toarray()], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(LSTM(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF8_PnaFv7Zw",
        "outputId": "d21954e7-6705-4239-a016-8f1d694ed4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 0.9379 - accuracy: 0.6273 - val_loss: 0.7473 - val_accuracy: 0.7480\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.5976 - accuracy: 0.7912 - val_loss: 0.5076 - val_accuracy: 0.8210\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4525 - accuracy: 0.8382 - val_loss: 0.4662 - val_accuracy: 0.8320\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3859 - accuracy: 0.8627 - val_loss: 0.4507 - val_accuracy: 0.8490\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3498 - accuracy: 0.8740 - val_loss: 0.4474 - val_accuracy: 0.8430\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2943 - accuracy: 0.8925 - val_loss: 0.4605 - val_accuracy: 0.8440\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2773 - accuracy: 0.9022 - val_loss: 0.4414 - val_accuracy: 0.8520\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2244 - accuracy: 0.9160 - val_loss: 0.4514 - val_accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.1992 - accuracy: 0.9317 - val_loss: 0.5045 - val_accuracy: 0.8450\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.1789 - accuracy: 0.9373 - val_loss: 0.5408 - val_accuracy: 0.8280\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.86      0.88       647\n",
            "           1       0.83      0.84      0.84       163\n",
            "           2       0.68      0.73      0.71       176\n",
            "           3       0.18      0.43      0.25        14\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.65      0.72      0.67      1000\n",
            "weighted avg       0.85      0.83      0.84      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W2V CBOW with GRU & LSTM"
      ],
      "metadata": {
        "id": "pf4Vd4i1H-_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m90vxKzAILL",
        "outputId": "a735c527-601c-4579-ab25-6c7fc5875852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 0.9311 - accuracy: 0.6398 - val_loss: 0.7486 - val_accuracy: 0.7130\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.7588 - val_loss: 0.5360 - val_accuracy: 0.8000\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4766 - accuracy: 0.8235 - val_loss: 0.4638 - val_accuracy: 0.8410\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3750 - accuracy: 0.8662 - val_loss: 0.4569 - val_accuracy: 0.8460\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3001 - accuracy: 0.8895 - val_loss: 0.5192 - val_accuracy: 0.8190\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2487 - accuracy: 0.9087 - val_loss: 0.4380 - val_accuracy: 0.8640\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1636 - accuracy: 0.9460 - val_loss: 0.4493 - val_accuracy: 0.8580\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1140 - accuracy: 0.9628 - val_loss: 0.4635 - val_accuracy: 0.8680\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0655 - accuracy: 0.9827 - val_loss: 0.4691 - val_accuracy: 0.8650\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0511 - accuracy: 0.9865 - val_loss: 0.5359 - val_accuracy: 0.8660\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.89      0.91       647\n",
            "           1       0.89      0.85      0.87       163\n",
            "           2       0.72      0.82      0.76       176\n",
            "           3       0.26      0.36      0.30        14\n",
            "\n",
            "    accuracy                           0.87      1000\n",
            "   macro avg       0.70      0.73      0.71      1000\n",
            "weighted avg       0.87      0.87      0.87      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "X_train_w2v_c_embeddings_array = np.array(X_train_w2v_c_embeddings_list)\n",
        "word2vec_embeddings = X_train_w2v_c_embeddings_array.reshape((4000,300))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 300, weights = [word2vec_embeddings], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(LSTM(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "X_train_w2v_c_embeddings_array = np.array(X_train_w2v_c_embeddings_list)\n",
        "word2vec_embeddings = X_train_w2v_c_embeddings_array.reshape((4000,300))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 300, weights = [word2vec_embeddings], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(GRU(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "id": "97DTaeYc7gAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa402c9-d6d3-415a-b62e-930baf5580ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 0.9349 - accuracy: 0.6292 - val_loss: 0.7297 - val_accuracy: 0.7410\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6255 - accuracy: 0.7670 - val_loss: 0.5074 - val_accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4432 - accuracy: 0.8380 - val_loss: 0.5201 - val_accuracy: 0.8110\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8835 - val_loss: 0.4119 - val_accuracy: 0.8580\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2318 - accuracy: 0.9165 - val_loss: 0.4068 - val_accuracy: 0.8630\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1623 - accuracy: 0.9450 - val_loss: 0.4863 - val_accuracy: 0.8590\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1082 - accuracy: 0.9653 - val_loss: 0.4428 - val_accuracy: 0.8760\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0771 - accuracy: 0.9770 - val_loss: 0.4743 - val_accuracy: 0.8660\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0334 - accuracy: 0.9933 - val_loss: 0.5098 - val_accuracy: 0.8640\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0335 - accuracy: 0.9918 - val_loss: 0.5076 - val_accuracy: 0.8850\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.92       647\n",
            "           1       0.90      0.89      0.89       163\n",
            "           2       0.80      0.74      0.77       176\n",
            "           3       0.57      0.29      0.38        14\n",
            "\n",
            "    accuracy                           0.89      1000\n",
            "   macro avg       0.79      0.71      0.74      1000\n",
            "weighted avg       0.88      0.89      0.88      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W2V SG with GRU & LSTM"
      ],
      "metadata": {
        "id": "xZq1FzHTICz0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F8KObZDAINJ",
        "outputId": "2b96b2aa-4088-46bb-debb-1a7827d8eb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 4s 9ms/step - loss: 0.9893 - accuracy: 0.6130 - val_loss: 0.8655 - val_accuracy: 0.6580\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.8682 - accuracy: 0.6587 - val_loss: 0.7663 - val_accuracy: 0.6960\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.7049 - accuracy: 0.7362 - val_loss: 0.5980 - val_accuracy: 0.7820\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5785 - accuracy: 0.7908 - val_loss: 0.5627 - val_accuracy: 0.7980\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5052 - accuracy: 0.8163 - val_loss: 0.5047 - val_accuracy: 0.8210\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4415 - accuracy: 0.8465 - val_loss: 0.4974 - val_accuracy: 0.8210\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4069 - accuracy: 0.8505 - val_loss: 0.5315 - val_accuracy: 0.8150\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3600 - accuracy: 0.8727 - val_loss: 0.4952 - val_accuracy: 0.8230\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3059 - accuracy: 0.8928 - val_loss: 0.4818 - val_accuracy: 0.8290\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.2720 - accuracy: 0.9032 - val_loss: 0.4976 - val_accuracy: 0.8370\n",
            "32/32 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.90       647\n",
            "           1       0.72      0.85      0.78       163\n",
            "           2       0.76      0.72      0.74       176\n",
            "           3       0.08      0.07      0.08        14\n",
            "\n",
            "    accuracy                           0.84      1000\n",
            "   macro avg       0.62      0.63      0.62      1000\n",
            "weighted avg       0.84      0.84      0.84      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "X_train_w2v_sg_embeddings_array = np.array(X_train_w2v_sg_embeddings_list)\n",
        "word2vec_embeddings = X_train_w2v_sg_embeddings_array.reshape((4000,300))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 300, weights = [word2vec_embeddings], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(GRU(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "x = pd.DataFrame(df['News_processed'])\n",
        "labels =  df['Type'].replace({'politics': 0, 'sport':1, 'economic':2, 'tech':3})\n",
        "y = to_categorical(labels)\n",
        "\n",
        "y2 = le.fit_transform(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
        "X_train_w2v_sg_embeddings_array = np.array(X_train_w2v_sg_embeddings_list)\n",
        "word2vec_embeddings = X_train_w2v_sg_embeddings_array.reshape((4000,300))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=4000)\n",
        "tokenizer.fit_on_texts(x_train['News_processed'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train['News_processed'])\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test['News_processed'])\n",
        "\n",
        "max_sequence_len = max(len(x) for x in train_sequences + test_sequences)\n",
        "\n",
        "x_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "x_val_padded = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(4000, 300, weights = [word2vec_embeddings], input_length=max_sequence_len, trainable=False))\n",
        "\n",
        "model.add(LSTM(150))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, epochs=10, verbose=1, validation_data=(x_val_padded, y_test))\n",
        "\n",
        "y_pred = model.predict(x_val_padded)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "id": "oJ4bN2Alm9Oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1df610d-f67c-4dee-aa5c-386384019b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 3s 9ms/step - loss: 1.0062 - accuracy: 0.6035 - val_loss: 0.9246 - val_accuracy: 0.6470\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.8747 - accuracy: 0.6507 - val_loss: 0.8252 - val_accuracy: 0.6780\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.7140 - val_loss: 0.7051 - val_accuracy: 0.7380\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.6763 - accuracy: 0.7542 - val_loss: 0.6506 - val_accuracy: 0.7850\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6141 - accuracy: 0.7775 - val_loss: 0.5879 - val_accuracy: 0.7990\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5739 - accuracy: 0.7933 - val_loss: 0.5862 - val_accuracy: 0.7840\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.5553 - accuracy: 0.7975 - val_loss: 0.6368 - val_accuracy: 0.7540\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5177 - accuracy: 0.8130 - val_loss: 0.5303 - val_accuracy: 0.8070\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4766 - accuracy: 0.8325 - val_loss: 0.5102 - val_accuracy: 0.8200\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4643 - accuracy: 0.8335 - val_loss: 0.5521 - val_accuracy: 0.8150\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88       647\n",
            "           1       0.65      0.87      0.75       163\n",
            "           2       0.74      0.62      0.67       176\n",
            "           3       0.00      0.00      0.00        14\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.57      0.59      0.58      1000\n",
            "weighted avg       0.81      0.81      0.81      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLDNHJXanAKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4n4Jdt8AIQw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPvgQGZfcYkk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}